{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "028f1144",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-02 09:53:10.937541: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-05-02 09:53:16.069905: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-05-02 09:53:16.070201: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-05-02 09:53:16.083454: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-05-02 09:53:17.890833: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-02 09:53:34.344272: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/cluster/home/taheeraa/.local/lib/python3.9/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.8.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "/cluster/home/taheeraa/.local/lib/python3.9/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from torchvision.models import efficientnet_b1\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7cd9394e",
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_dir = os.getcwd()\n",
    "project_dir = os.path.dirname(notebook_dir)\n",
    "\n",
    "# Add the src directory to sys.path\n",
    "src_dir = os.path.join(project_dir, 'src')\n",
    "if src_dir not in sys.path:\n",
    "    sys.path.append(src_dir)\n",
    "\n",
    "from utils import generate_class_weights, show_batch_images\n",
    "from data import ChestXray14HFDataset\n",
    "from trainers import MultiLabelLightningModule\n",
    "from models import set_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795136ff",
   "metadata": {},
   "source": [
    "## load dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad8813ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/cluster/home/taheeraa/datasets/chestxray-14\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de5e7930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights: {'Atelectasis': 0.06666666666666667, 'Cardiomegaly': 0.6968494101318529, 'Effusion': 3.3449700199866754, 'Infiltration': 0.6563995293502418, 'Mass': 0.41714855433698905, 'Nodule': 1.4048125349748182, 'Pneumonia': 1.2366502463054188, 'Pneumothorax': 6.52051948051948, 'Consolidation': 2.237433155080214, 'Edema': 2.024516129032258, 'Emphysema': 4.1494214876033055, 'Fibrosis': 4.01664, 'Pleural_Thickening': 4.63601108033241, 'Hernia': 2.6055007784120394}\n",
      "Train dataframe shape: (75312, 16) (1 size larger than expected due to 'Full Image Path')\n",
      "Train columns: Index(['Full Image Path', 'Image Filename', 'Atelectasis', 'Cardiomegaly',\n",
      "       'Effusion', 'Infiltration', 'Mass', 'Nodule', 'Pneumonia',\n",
      "       'Pneumothorax', 'Consolidation', 'Edema', 'Emphysema', 'Fibrosis',\n",
      "       'Pleural_Thickening', 'Hernia'],\n",
      "      dtype='object')\n",
      "Labels: ['Atelectasis', 'Cardiomegaly', 'Effusion', 'Infiltration', 'Mass', 'Nodule', 'Pneumonia', 'Pneumothorax', 'Consolidation', 'Edema', 'Emphysema', 'Fibrosis', 'Pleural_Thickening', 'Hernia']\n",
      "Number of labels: 14\n"
     ]
    }
   ],
   "source": [
    "labels = [\n",
    "    \"Atelectasis\", \n",
    "    \"Cardiomegaly\",\n",
    "    \"Effusion\", \n",
    "    \"Infiltration\", \n",
    "    \"Mass\",\n",
    "    \"Nodule\",\n",
    "    \"Pneumonia\",\n",
    "    \"Pneumothorax\",  \n",
    "    \"Consolidation\",\n",
    "    \"Edema\",\n",
    "    \"Emphysema\",\n",
    "    \"Fibrosis\",\n",
    "    \"Pleural_Thickening\",\n",
    "    \"Hernia\"\n",
    "]\n",
    "num_labels = len(labels)\n",
    "\n",
    "file_path_train = data_path + '/train_official.txt'\n",
    "file_path_val = data_path + '/val_official.txt'\n",
    "file_path_test = data_path + '/test_official.txt'\n",
    "\n",
    "columns = ['Image Filename'] + labels\n",
    "\n",
    "df_train = pd.read_csv(file_path_train, sep='\\s+', names=columns)\n",
    "df_val = pd.read_csv(file_path_val, sep='\\s+', names=columns)\n",
    "df_test = pd.read_csv(file_path_test, sep='\\s+', names=columns)\n",
    "\n",
    "# Finding all image paths, and mapping them to the DataFrame\n",
    "subfolders = [f\"images_{i:03}/images\" for i in range(1, 13)]  # Generates 'images_001' to 'images_012'\n",
    "path_mapping = {}\n",
    "for subfolder in subfolders:\n",
    "    full_folder_path = os.path.join(data_path, subfolder)\n",
    "    for img_file in os.listdir(full_folder_path):\n",
    "        path_mapping[img_file] = os.path.join(full_folder_path, img_file)\n",
    "\n",
    "# Update the DataFrame using the mapping\n",
    "df_train['Full Image Path'] = df_train['Image Filename'].map(path_mapping)\n",
    "df_val['Full Image Path'] = df_val['Image Filename'].map(path_mapping)\n",
    "df_test['Full Image Path'] = df_test['Image Filename'].map(path_mapping)\n",
    "\n",
    "# Move 'Full Image Path' to the front of the DataFrame\n",
    "cols_train = ['Full Image Path'] + [col for col in df_train.columns if col != 'Full Image Path']\n",
    "cols_val = ['Full Image Path'] + [col for col in df_val.columns if col != 'Full Image Path']\n",
    "cols_test = ['Full Image Path'] + [col for col in df_test.columns if col != 'Full Image Path']\n",
    "df_train = df_train[cols_train]\n",
    "df_val = df_val[cols_val]\n",
    "df_test = df_test[cols_test]\n",
    "\n",
    "# Drop 'Image Filename' column\n",
    "train_df = df_train.drop(columns=['Image Filename'])\n",
    "val_df = df_val.drop(columns=['Image Filename'])\n",
    "test_df = df_test.drop(columns=['Image Filename'])\n",
    "\n",
    "# Create class weights\n",
    "df_train_calculate_weights = df_train.drop(columns=['Full Image Path']).to_numpy()\n",
    "class_weights_dict = generate_class_weights(df_train_calculate_weights, multi_class=False, one_hot_encoded=True)\n",
    "class_weights_list = [class_weights_dict[i] for i in class_weights_dict]\n",
    "class_weights_tensor = torch.tensor(class_weights_list, dtype=torch.float32)\n",
    "\n",
    "label_weights_dict = {labels[i]: class_weights_dict[i] for i in range(len(labels))}\n",
    "print(f\"Class weights: {label_weights_dict}\")\n",
    "\n",
    "print(f\"Train dataframe shape: {df_train.shape} (1 size larger than expected due to 'Full Image Path')\")\n",
    "print(f\"Train columns: {df_train.columns}\")\n",
    "print(f\"Labels: {labels}\")\n",
    "print(f\"Number of labels: {len(labels)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d4b73c",
   "metadata": {},
   "source": [
    "## load dataset and transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e244f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 224\n",
    "num_workers = 4\n",
    "pin_memory = False\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1901e3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((256, 256), interpolation=transforms.InterpolationMode.BILINEAR, antialias=True),\n",
    "    transforms.CenterCrop(img_size),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=7),\n",
    "    transforms.ToTensor(),\n",
    "    normalize,\n",
    "])\n",
    "\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.Resize((256, 256), interpolation=transforms.InterpolationMode.BILINEAR, antialias=True),\n",
    "    transforms.CenterCrop(img_size),\n",
    "    transforms.ToTensor(),\n",
    "    normalize,\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6582849",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ChestXray14HFDataset(\n",
    "    dataframe=train_df, transform=train_transforms)\n",
    "val_dataset = ChestXray14HFDataset(\n",
    "    dataframe=val_df, transform=val_transforms)\n",
    "test_dataset = ChestXray14HFDataset(\n",
    "    dataframe=test_df, transform=val_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4e050b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True, \n",
    "    num_workers=num_workers, \n",
    "    pin_memory=pin_memory\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=False, \n",
    "    num_workers=num_workers, \n",
    "    pin_memory=pin_memory\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=False, \n",
    "    num_workers=num_workers, \n",
    "    pin_memory=pin_memory\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62b8f3c",
   "metadata": {},
   "source": [
    "## load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e6f5f879",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file=\"/cluster/home/taheeraa/code/master-thesis/01-multi-label/checkpoints/DenseNet121_aug4_pretrain_WeightBelow1_1_0.829766922537.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "914b7ee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before: The model has 7794184 trainable parameters\n",
      "after: The model has 7370030 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "model, img_size = set_model('resnet50', num_labels, labels)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "learning_rate = 0.001\n",
    "optimizer_func = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler_func = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_func, mode='max', factor=0.1, patience=3, verbose=True)\n",
    "model_ckpts_folder = \"/cluster/home/taheeraa/code/master-thesis/01-multi-label/notebooks/checkpoints\"\n",
    "logger = None\n",
    "root_path = \"/cluster/home/taheeraa/code/master-thesis/01-multi-label/output\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "training_module = MultiLabelLightningModule(\n",
    "    model=model,\n",
    "    criterion=criterion,\n",
    "    learning_rate=learning_rate,\n",
    "    num_labels=num_labels,\n",
    "    labels=labels,\n",
    "    optimizer_func=optimizer_func,\n",
    "    scheduler_func=scheduler_func,\n",
    "    model_ckpts_folder=model_ckpts_folder,\n",
    "    logger=logger,\n",
    "    root_path=root_path,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
