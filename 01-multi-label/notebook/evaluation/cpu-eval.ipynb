{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import timm\n",
    "import random\n",
    "import torch.nn as nn\n",
    "from memory_profiler import memory_usage\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import copy\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate CPU inference time\n",
    "\n",
    "Want to add gradcam to alexnet baseline and compare them to the bounding boxes in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_folder = '/cluster/home/taheeraa/datasets/chestxray-14/'\n",
    "images_path = f\"{root_folder}images\"\n",
    "file_path_bbox = root_folder + 'BBox_List_2017.csv'\n",
    "\n",
    "labels = [\n",
    "        \"Atelectasis\", \n",
    "        \"Cardiomegaly\",\n",
    "        \"Effusion\", \n",
    "        \"Infiltration\", \n",
    "        \"Mass\",\n",
    "        \"Nodule\",\n",
    "        \"Pneumonia\",\n",
    "        \"Pneumothorax\",  \n",
    "        \"Consolidation\",\n",
    "        \"Edema\",\n",
    "        \"Emphysema\",\n",
    "        \"Fibrosis\",\n",
    "        \"Pleural_Thickening\",\n",
    "        \"Hernia\"\n",
    "    ]\n",
    "num_labels = len(labels) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/python\n"
     ]
    }
   ],
   "source": [
    "!which python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/cluster/home/taheeraa/code/BenchmarkTransformers/models/classification/ChestXray14/06-transformers-pre-trained/swin_base_imagenet_1k_sgd_64_bce_aug/model.pth.tar'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base = \"/cluster/home/taheeraa/code/BenchmarkTransformers/models/classification/ChestXray14/\"\n",
    "experiment_name = '06-transformers-pre-trained'\n",
    "type = 'swin_base_imagenet_1k_sgd_64_bce_aug'\n",
    "\n",
    "# experiment_name = \"07-transformer-ssl/\"\n",
    "# type = \"swin_base_simmim\"\n",
    "pretrained_weights = os.path.join(base, experiment_name, type, \"model.pth.tar\")\n",
    "model_str = 'swin_in22k'\n",
    "batch_size = 32\n",
    "pretrained_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")  # Use GPU\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loading model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded with msg: <All keys matched successfully>\n"
     ]
    }
   ],
   "source": [
    "def classifying_head(in_features: int, num_labels: int):\n",
    "    return nn.Sequential(\n",
    "        nn.Dropout(p=0.2),\n",
    "        nn.Linear(in_features=in_features, out_features=128),\n",
    "        nn.ReLU(),\n",
    "        nn.BatchNorm1d(num_features=128),\n",
    "        nn.Linear(128, num_labels),\n",
    "    )\n",
    "\n",
    "def load_model(pretrained_weights, num_labels, model_str):\n",
    "    \n",
    "    checkpoint = torch.load(\n",
    "        pretrained_weights, map_location=torch.device('cpu'))\n",
    "    state_dict = checkpoint['state_dict']\n",
    "\n",
    "    if model_str == \"densenet121\":\n",
    "        model = timm.create_model(\n",
    "            'densenet121', num_classes=num_labels, pretrained=True)\n",
    "        model.classifier = classifying_head(1024, num_labels)\n",
    "    elif model_str == \"swin_simim\" or model_str == \"swin_in22k\":\n",
    "        model = timm.create_model(\n",
    "            'swin_base_patch4_window7_224_in22k', num_classes=num_labels, pretrained=True)\n",
    "    elif model_str == \"vit_in1k\":\n",
    "        model = timm.create_model('vit_base_patch16_224',\n",
    "                                  num_classes=num_labels, pretrained=True)\n",
    "\n",
    "    if model_str == \"swin_simim\":\n",
    "        normalization = \"chestx-ray\"\n",
    "    else: normalization = \"imagenet\"\n",
    "\n",
    "    checkpoint = torch.load(pretrained_weights, map_location=\"cpu\")\n",
    "\n",
    "    state_dict = checkpoint['state_dict']\n",
    "    msg = model.load_state_dict(state_dict, strict=False)\n",
    "    print('Loaded with msg: {}'.format(msg))\n",
    "    return model, normalization\n",
    "\n",
    "model, normalization = load_model(pretrained_weights, num_labels, model_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_transform_classification(normalize, crop_size=224, resize=256, mode=\"train\", test_augment=True):\n",
    "    transformations_list = []\n",
    "\n",
    "    if normalize.lower() == \"imagenet\":\n",
    "      normalize = transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    elif normalize.lower() == \"chestx-ray\":\n",
    "      normalize = transforms.Normalize([0.5056, 0.5056, 0.5056], [0.252, 0.252, 0.252])\n",
    "    elif normalize.lower() == \"none\":\n",
    "      normalize = None\n",
    "    else:\n",
    "      print(\"mean and std for [{}] dataset do not exist!\".format(normalize))\n",
    "      exit(-1)\n",
    "    if mode == \"train\":\n",
    "      transformations_list.append(transforms.RandomResizedCrop(crop_size))\n",
    "      transformations_list.append(transforms.RandomHorizontalFlip())\n",
    "      transformations_list.append(transforms.RandomRotation(7))\n",
    "      transformations_list.append(transforms.ToTensor())\n",
    "      if normalize is not None:\n",
    "        transformations_list.append(normalize)\n",
    "    elif mode == \"valid\":\n",
    "      transformations_list.append(transforms.Resize((resize, resize)))\n",
    "      transformations_list.append(transforms.CenterCrop(crop_size))\n",
    "      transformations_list.append(transforms.ToTensor())\n",
    "      if normalize is not None:\n",
    "        transformations_list.append(normalize)\n",
    "    elif mode == \"test\":\n",
    "      if test_augment:\n",
    "        transformations_list.append(transforms.Resize((resize, resize)))\n",
    "        transformations_list.append(transforms.TenCrop(crop_size))\n",
    "        transformations_list.append(\n",
    "          transforms.Lambda(lambda crops: torch.stack([transforms.ToTensor()(crop) for crop in crops])))\n",
    "        if normalize is not None:\n",
    "          transformations_list.append(transforms.Lambda(lambda crops: torch.stack([normalize(crop) for crop in crops])))\n",
    "      else:\n",
    "        transformations_list.append(transforms.Resize((resize, resize)))\n",
    "        transformations_list.append(transforms.CenterCrop(crop_size))\n",
    "        transformations_list.append(transforms.ToTensor())\n",
    "        if normalize is not None:\n",
    "          transformations_list.append(normalize)\n",
    "    transformSequence = transforms.Compose(transformations_list)\n",
    "\n",
    "    return transformSequence\n",
    "\n",
    "class ChestXray14Dataset(Dataset):\n",
    "  def __init__(self, images_path, file_path, augment, num_class=14, annotation_percent=100):\n",
    "\n",
    "    self.img_list = []\n",
    "    self.img_label = []\n",
    "    self.augment = augment\n",
    "\n",
    "    with open(file_path, \"r\") as fileDescriptor:\n",
    "      line = True\n",
    "\n",
    "      while line:\n",
    "        line = fileDescriptor.readline()\n",
    "\n",
    "        if line:\n",
    "          lineItems = line.split()\n",
    "\n",
    "          imagePath = os.path.join(images_path, lineItems[0])\n",
    "          imageLabel = lineItems[1:num_class + 1]\n",
    "          imageLabel = [int(i) for i in imageLabel]\n",
    "\n",
    "          self.img_list.append(imagePath)\n",
    "          self.img_label.append(imageLabel)\n",
    "\n",
    "    indexes = np.arange(len(self.img_list))\n",
    "    if annotation_percent < 100:\n",
    "      random.Random(99).shuffle(indexes)\n",
    "      num_data = int(indexes.shape[0] * annotation_percent / 100.0)\n",
    "      indexes = indexes[:num_data]\n",
    "\n",
    "      _img_list, _img_label = copy.deepcopy(self.img_list), copy.deepcopy(self.img_label)\n",
    "      self.img_list = []\n",
    "      self.img_label = []\n",
    "\n",
    "      for i in indexes:\n",
    "        self.img_list.append(_img_list[i])\n",
    "        self.img_label.append(_img_label[i])\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "\n",
    "    imagePath = self.img_list[index]\n",
    "\n",
    "    imageData = Image.open(imagePath).convert('RGB')\n",
    "    imageLabel = torch.FloatTensor(self.img_label[index])\n",
    "\n",
    "    if self.augment != None: imageData = self.augment(imageData)\n",
    "\n",
    "    return {'pixel_values': imageData, 'labels': imageLabel}\n",
    "\n",
    "  def __len__(self):\n",
    "\n",
    "    return len(self.img_list)\n",
    "  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transforms = build_transform_classification(\n",
    "        mode = \"test\",\n",
    "        normalize=normalization,\n",
    "        test_augment=True,\n",
    "    )\n",
    "\n",
    "path_to_labels = '/cluster/home/taheeraa/code/BenchmarkTransformers/dataset'\n",
    "file_path_train = path_to_labels + '/Xray14_train_official.txt'\n",
    "file_path_val = path_to_labels + '/Xray14_val_official.txt'\n",
    "file_path_test = path_to_labels + '/Xray14_test_official.txt'\n",
    "\n",
    "test_dataset = ChestXray14Dataset(images_path=images_path, file_path=file_path_test,\n",
    "                                      augment=test_transforms, num_class=num_labels)\n",
    "\n",
    "\n",
    "test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=32,\n",
    "        shuffle=False,\n",
    "        num_workers=8,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SwinTransformer(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv2d(3, 128, kernel_size=(4, 4), stride=(4, 4))\n",
       "    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "  (layers): Sequential(\n",
       "    (0): BasicLayer(\n",
       "      dim=128, input_resolution=(56, 56), depth=2\n",
       "      (blocks): ModuleList(\n",
       "        (0): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): DropPath()\n",
       "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (downsample): PatchMerging(\n",
       "        input_resolution=(56, 56), dim=128\n",
       "        (reduction): Linear(in_features=512, out_features=256, bias=False)\n",
       "        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicLayer(\n",
       "      dim=256, input_resolution=(28, 28), depth=2\n",
       "      (blocks): ModuleList(\n",
       "        (0-1): 2 x SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): DropPath()\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (downsample): PatchMerging(\n",
       "        input_resolution=(28, 28), dim=256\n",
       "        (reduction): Linear(in_features=1024, out_features=512, bias=False)\n",
       "        (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (2): BasicLayer(\n",
       "      dim=512, input_resolution=(14, 14), depth=18\n",
       "      (blocks): ModuleList(\n",
       "        (0-17): 18 x SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): DropPath()\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (downsample): PatchMerging(\n",
       "        input_resolution=(14, 14), dim=512\n",
       "        (reduction): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "        (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (3): BasicLayer(\n",
       "      dim=1024, input_resolution=(7, 7), depth=2\n",
       "      (blocks): ModuleList(\n",
       "        (0-1): 2 x SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): DropPath()\n",
       "          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  (avgpool): AdaptiveAvgPool1d(output_size=1)\n",
       "  (head): Linear(in_features=1024, out_features=14, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "model.to(device)\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/800 [05:28<36:29:04, 164.59s/it]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "import psutil\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Ensure everything is on the CPU\n",
    "device = torch.device('cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "# Prepare the test tensors\n",
    "y_test = torch.FloatTensor().to(device)\n",
    "p_test = torch.FloatTensor().to(device)\n",
    "\n",
    "# Variables to store memory usage and inference time\n",
    "memory_usage = []\n",
    "inference_times = []\n",
    "\n",
    "process = psutil.Process()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, batch in enumerate(tqdm(test_loader)):\n",
    "        samples = batch['pixel_values']\n",
    "        targets = batch['labels']\n",
    "        \n",
    "        targets = targets.to(device)\n",
    "        y_test = torch.cat((y_test, targets), 0)\n",
    "\n",
    "        if len(samples.size()) == 4:\n",
    "            bs, c, h, w = samples.size()\n",
    "            n_crops = 1\n",
    "        elif len(samples.size()) == 5:\n",
    "            bs, n_crops, c, h, w = samples.size()\n",
    "\n",
    "        varInput = torch.autograd.Variable(\n",
    "            samples.view(-1, c, h, w).to(device))\n",
    "\n",
    "        # Measure inference time\n",
    "        start_time = time.time()\n",
    "        out = model(varInput)\n",
    "        out = torch.sigmoid(out)\n",
    "        inference_time = time.time() - start_time\n",
    "\n",
    "        outMean = out.view(bs, n_crops, -1).mean(1)\n",
    "        p_test = torch.cat((p_test, outMean.data), 0)\n",
    "\n",
    "        # Record inference time\n",
    "        inference_times.append(inference_time)\n",
    "\n",
    "        # Measure memory usage\n",
    "        memory_info = process.memory_info()\n",
    "        memory_usage.append(memory_info.rss)\n",
    "\n",
    "# Calculate average inference time and memory usage\n",
    "average_inference_time = sum(inference_times) / len(inference_times)\n",
    "average_memory_usage = sum(memory_usage) / len(memory_usage)\n",
    "\n",
    "print(f'Average inference time: {average_inference_time:.6f} seconds')\n",
    "print(f'Average memory usage: {average_memory_usage / (1024 ** 2):.2f} MB')\n",
    "\n",
    "# Save the results to a CSV file\n",
    "results = pd.DataFrame({\n",
    "    'Inference Time (s)': inference_times,\n",
    "    'Memory Usage (bytes)': memory_usage\n",
    "})\n",
    "\n",
    "results.to_csv(f'{model_str}_inference_memory_usage.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
